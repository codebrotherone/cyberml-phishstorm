{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reload' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a37a93b19601>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefaultencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reload' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "reload(sys) \n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 18259: expected 14 fields, saw 15\\nSkipping line 18273: expected 14 fields, saw 15\\n'\n",
      "C:\\Users\\codebrotherone\\.conda\\envs\\cyber\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (1,2,3,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    io.StringIO(\n",
    "        open(\"urlset.csv\", errors=\"ignore\").read()), \n",
    "            error_bad_lines=False, \n",
    "#             escapechar='\\\\'\n",
    "    )\n",
    "\n",
    "disc = df.sample(frac=0.5)\n",
    "gan =df[~df.index.isin(disc.index)]\n",
    "\n",
    "gan_xtrain, gan_xtest, gan_ytrain, gan_ytest = train_test_split(\n",
    "    gan.values[:, 1:12], gan.values[:, 13], test_size=.33)\n",
    "\n",
    "disc_xtrain, disc_xtest, disc_ytrain, disc_ytest = train_test_split(\n",
    "    disc.values[:, 1:12], disc.values[:, 13], )\n",
    "\n",
    "disc = [disc_xtrain, disc_xtest, disc_ytrain, disc_ytest]\n",
    "gan = [gan_xtrain, gan_xtest, gan_ytrain, gan_ytest]\n",
    "\n",
    "\n",
    "# df=df.sample(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for col in df.columns.values[1:13]:\n",
    "\n",
    "    df[col] = df[col].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "    df = df[~df[col].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 1.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 8.57143000e-01, 6.45283000e-01],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.61397000e-01],\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 6.00000000e-01],\n",
       "       ...,\n",
       "       [1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.30297000e-01],\n",
       "       [3.83200000e-04, 1.00000000e+00, 1.42857143e-01, ...,\n",
       "        0.00000000e+00, 5.16129000e-01, 0.00000000e+00],\n",
       "       [9.35600000e-04, 1.00000000e+00, 1.42857143e-01, ...,\n",
       "        6.81820000e-02, 6.47059000e-01, 7.77778000e-01]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get data\n",
    "data = df.values[:,1:]\n",
    "x_train, x_test, y_train, y_test= train_test_split(data[:, :12], data[:, 12])\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_xtrain = scaler.fit_transform(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# You will use the Adam optimizer\n",
    "def get_optimizer():\n",
    "    return Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "\n",
    "def create_gen_model():\n",
    "    \"\"\"This will create the generator model which will use tanh activation for output\n",
    "    Notes:\n",
    "        https://stackoverflow.com/questions/41489907/generative-adversarial-networks-tanh\n",
    "    Args:\n",
    "        input_dim (tuple): shape representing inputs to feed\n",
    "        opt (keras.optimizers): optimizer to use (adam by default)\n",
    "    Returns:\n",
    "        generator (keras.SequentialModel): generator model\n",
    "    \"\"\"\n",
    "    optimizer=get_optimizer()\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(128, input_dim=100))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "\n",
    "    generator.add(Dense(256))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "\n",
    "    generator.add(Dense(512))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "\n",
    "    generator.add(Dense(12, activation='tanh'))\n",
    "    \n",
    "    generator.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return generator\n",
    "\n",
    "\n",
    "# discriminator\n",
    "def create_disc_model():\n",
    "    \"\"\"This will create a discriminator model for our GAN network using Keras\n",
    "    Args:\n",
    "        Optimizer (keras.optimizers): Adam\n",
    "    Returns:\n",
    "        discriminator (keras.SequentialModel): discriminator model\n",
    "    \"\"\"\n",
    "    optimizer=get_optimizer()\n",
    "    discriminator = Sequential()\n",
    "    # first layer\n",
    "    discriminator.add(Dense(64, input_dim=12))\n",
    "    discriminator.add(LeakyReLU(.2))\n",
    "    discriminator.add(Dropout(.3))\n",
    "    # second layer\n",
    "    discriminator.add(Dense(128))\n",
    "    discriminator.add(LeakyReLU(.2))\n",
    "    discriminator.add(Dropout(.3))\n",
    "    # third layer\n",
    "    discriminator.add(Dense(64))\n",
    "    discriminator.add(LeakyReLU(.2))\n",
    "    discriminator.add(Dropout(.3))\n",
    "\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return discriminator\n",
    "\n",
    "\n",
    "def create_gan_network(discriminator, random_dim, generator, optimizer):\n",
    "    # We initially set trainable to False since we only want to train either the\n",
    "    # generator or discriminator at a time\n",
    "    discriminator.trainable = False\n",
    "    # gan input (noise) will be 100-dimensional vectors\n",
    "    gan_input = Input(shape=(random_dim,))\n",
    "    # the output of the generator (an image)\n",
    "    x = generator(gan_input)\n",
    "    # get the output of the discriminator (probability if the image is real or not)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return gan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gan = create_gan_network(discriminator, random_dim, generator, adam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                         | 0/3596 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\codebrotherone\\.conda\\envs\\cyber\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3596/3596 [00:25<00:00, 141.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "# globals\n",
    "epochs=1\n",
    "batch_size=20\n",
    "random_dim = 100\n",
    "\n",
    "# Get the training and testing data\n",
    "# for generator and discriminator\n",
    "x_train = scaled_xtrain\n",
    "\n",
    "# Split the training data into batches of size 128\n",
    "batch_count = x_train.shape[0] / batch_size\n",
    "\n",
    "# Build our GAN netowrk\n",
    "adam = get_optimizer()\n",
    "generator = create_gen_model()\n",
    "discriminator = create_disc_model()\n",
    "\n",
    "\n",
    "gan = create_gan_network(discriminator, random_dim, generator, adam)\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "    for _ in tqdm(range(int(batch_count))):\n",
    "        # Get a random set of input noise and images\n",
    "        noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "        url_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
    "\n",
    "        # Generate fake url features\n",
    "        gen_urls = generator.predict(noise)\n",
    "        X = np.concatenate([url_batch, gen_urls])\n",
    "\n",
    "        # Labels for generated and real data\n",
    "        y_dis = np.zeros(2*batch_size)\n",
    "        # One-sided label smoothing\n",
    "        y_dis[:batch_size] = 1\n",
    "\n",
    "        # Train discriminator or generator\n",
    "        # based on below trainable attr()\n",
    "        discriminator.trainable = True\n",
    "        discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "        y_gen = np.ones(batch_size)\n",
    "        gan.train_on_batch(noise, y_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_233_input to have shape (100,) but got array with shape (12,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-7610341bba6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0murl_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_urls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\cyber\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cyber\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cyber\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_233_input to have shape (100,) but got array with shape (12,)"
     ]
    }
   ],
   "source": [
    "# Get a random set of input noise and images\n",
    "noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "url_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
    "# Generate fake url features\n",
    "gen_urls = generator.predict(noise)\n",
    "X = np.concatenate([url_batch, gen_urls])\n",
    "\n",
    "generator.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_233_input to have shape (100,) but got array with shape (12,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-157-642168c3e58f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\cyber\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1347\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1350\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cyber\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\cyber\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_233_input to have shape (100,) but got array with shape (12,)"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get data\n",
    "\n",
    "data = df.values[:, 1:]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(data[:, :12], data[:, 12])\n",
    "\n",
    "\n",
    "gan.evaluate(x_test, y_test, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random set of input noise and images\n",
    "noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "url_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\n",
    "\n",
    "# Generate fake url features\n",
    "gen_urls = generator.predict(noise)\n",
    "X = np.concatenate([url_batch, gen_urls])\n",
    "\n",
    "# Labels for generated and real data\n",
    "y_dis = np.zeros(2*batch_size)\n",
    "# One-sided label smoothing\n",
    "y_dis[:batch_size] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-6da2e4d573ea>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-6da2e4d573ea>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    return normalized\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    Takes dataframe and normalizes values between 0 and 1\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import normalize\n",
    "    for col in df.columns.values[1:13]:\n",
    "        df[col] = df[col].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "        df = df[~df[col].isnull()]\n",
    "        df[col] = df[col].apply(lambda x: normalize(np.array([x])))\n",
    "        \n",
    "    \n",
    "    normalized = normalize(np.array([df.iloc[:, 1:13].values]).reshape(df.shape, axis=1)\n",
    "    \n",
    "    return normalized\n",
    "                           \n",
    "def split_data(df):\n",
    "    train_test_split(df)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10000000.0, 1.0, 0.0, ..., 0.0, 0.0, 0.8],\n",
       "       [10000000.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "       [10000000.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "       ...,\n",
       "       [753.0, 1.0, 1.0, ..., 0.0, 0.0, 0.11111099999999999],\n",
       "       [6.0, 1.0, 1.0, ..., 0.03025, 0.029144999999999997, 0.809735],\n",
       "       [2547.0, 1.0, 1.0, ..., 0.017341, 0.020408000000000003, 0.636364]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values[:, 1:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95914, 14)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df.values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
